{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-vxnRyp64YuQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchvision.datasets\n",
        "from torch.utils import data\n",
        "from torchvision.datasets import utils\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import save_image\n",
        "import time\n",
        "import os\n",
        "import csv\n",
        "import torchsummary\n",
        "import io\n",
        "from contextlib import redirect_stdout\n",
        "from math import ceil\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YytEa-QZ5Q-T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###definition de nos hyperparametres"
      ]
    },
    {
      "metadata": {
        "id": "UJKi0DM-4mui",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#definition des hyperparametres\n",
        "batch_size = 64\n",
        "lr = 3e-4\n",
        "epochs = 20\n",
        "save_interval = 2\n",
        "log_interval = 100\n",
        "np.random.seed(1111)\n",
        "torch.random.manual_seed(1111)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "#Fontion de Chargement de la base de donnees Street View House Numbers (SVHN) \n",
        "#de maisons house numbers de Google Street View\n",
        "def loading_dataset_SVHN(dataset_location, batch_size):\n",
        "    image_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((.5, .5, .5),\n",
        "                             (.5, .5, .5))\n",
        "    ])\n",
        "\n",
        "    trainvalid = torchvision.datasets.SVHN(\n",
        "        dataset_location, split='train',\n",
        "        download=True,\n",
        "        transform=image_transform\n",
        "    )\n",
        "\n",
        "    trainset_size = int(len(trainvalid) * 0.9)\n",
        "    trainset, validset = data.dataset.random_split(\n",
        "        trainvalid,\n",
        "        [trainset_size, len(trainvalid) - trainset_size]\n",
        "    )\n",
        "\n",
        "    training_loader = data.DataLoader(\n",
        "        trainset,\n",
        "        batch_size=batch_size,\t\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    validation_loader = data.DataLoader(\n",
        "        validset,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    test_loader = data.DataLoader(\n",
        "        torchvision.datasets.SVHN(\n",
        "            dataset_location, split='test',\n",
        "            download=True,\n",
        "            transform=image_transform\n",
        "        ),\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    return training_loader, validation_loader, test_loader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jdpHlFcZ5rih",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####definition du modele VAE Q.3 ici in_channels=3 car le dataset SVHN est de dimension 3x32x32"
      ]
    },
    {
      "metadata": {
        "id": "WcDCUDxj57je",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(7, 7))\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=(2, 2), stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3))\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=(2, 2), stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=(5, 5))\n",
        "        self._mean = nn.Linear(in_features=256, out_features=100, bias=True)\n",
        "        self._logvar = nn.Linear(in_features=256, out_features=100, bias=True)\n",
        "        self.elu = nn.ELU()\n",
        "    def forward(self, input):\n",
        "        input = self.conv1(input)\n",
        "        input = self.elu(input)\n",
        "        input = self.pool1(input)\n",
        "        input = self.conv2(input)\n",
        "        input = self.elu(input)\n",
        "        input = self.pool2(input)\n",
        "        input = self.conv3(input)\n",
        "        input = self.elu(input)\n",
        "        input = input.view(input.size(0), -1)\n",
        "        mean = self._mean(input)\n",
        "        logvar = self._logvar(input)\n",
        "        return mean, logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.linear = nn.Linear(in_features=100, out_features=256, bias=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels=256, out_channels=64, kernel_size=(5, 5), padding=4)\n",
        "        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(3, 3), padding=2)\n",
        "        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(3, 3), padding=2)\n",
        "        self.conv4 = nn.Conv2d(in_channels=16, out_channels=3, kernel_size=(3, 3), padding=4)\n",
        "        self.elu = nn.ELU()\n",
        "    def forward(self, input):\n",
        "        input = self.linear(input)\n",
        "        input = input.unsqueeze(-1).unsqueeze(-1)\n",
        "        input = self.elu(input)\n",
        "        input = self.conv1(input)\n",
        "        input = self.elu(input)\n",
        "        input = self.upsample1(input)\n",
        "        input = self.conv2(input)\n",
        "        input = self.elu(input)\n",
        "        input = self.upsample2(input)\n",
        "        input = self.conv3(input)\n",
        "        input = self.elu(input)\n",
        "        input_tilde = self.conv4(input)\n",
        "        return input_tilde\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean, logvar = self.encoder(x)\n",
        "        epsilon = torch.randn_like(logvar)\n",
        "        z = mean + torch.exp(logvar / 2) * epsilon\n",
        "        input_tilde_logits = self.decoder(z)\n",
        "        return input_tilde_logits, mean, logvar\n",
        "\n",
        "\n",
        "#ELBO utilisant un decoder p(x|z) avec une distribution normale de variance 1\n",
        "class Elbo(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Elbo, self).__init__()\n",
        "\n",
        "    def forward(self, x, x_tilde, mean, logvar):\n",
        "        x = x.reshape((x.shape[0],-1))\n",
        "        x_tilde = x_tilde.reshape((x_tilde.shape[0], -1))\n",
        "        kl = 0.5 * (-logvar - 1 + mean**2 + torch.exp(logvar)).sum(dim=-1)\n",
        "        log_decoder = -0.5 * torch.sum((x - x_tilde)**2, -1) - 0.5 * x.shape[1] * np.log(2 * np.pi)\n",
        "        loss = -(log_decoder - kl).mean()\n",
        "        return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wghr8Xv97Mi2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Creation de la fonction de training (entrainement)"
      ]
    },
    {
      "metadata": {
        "id": "lU3REQQZ7QDK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# retourner l'entrainement de chaque epoque(une epoch)\n",
        "def epoch_training(model, optimizer, data_load, loss_function, epoch, log_interval=None):\n",
        "    model.train()\n",
        "    elbo = []\n",
        "    for batch_id, (x, _) in enumerate(data_load):\n",
        "        x = x.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        x_tilde_logits, mean, logvar = model(x)\n",
        "        loss = loss_function(x, x_tilde_logits, mean, logvar)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        elbo.append(-loss.item())\n",
        "        if log_interval is not None:\n",
        "            if batch_id%log_interval == 0:\n",
        "                print (f\"----> Epoch {epoch},\\t Batch {batch_id},\\t Training ELBO: {np.mean(np.array(elbo)):.2f}\")\n",
        "    return np.mean(np.array(elbo))\n",
        "\n",
        "# faire l'evaluation du modele dans les data \n",
        "def epoch_validation(model, data_load, loss_function):\n",
        "    model.eval()\n",
        "    elbo = []\n",
        "    for batch_id, (x, _) in enumerate(data_load):\n",
        "        x = x.to(device)\n",
        "        x_tilde_logits, mean, logvar = model(x)\n",
        "        loss = loss_function(x, x_tilde_logits, mean, logvar)\n",
        "        elbo.append(-loss.item())\n",
        "    return np.mean(np.array(elbo))\n",
        "\n",
        "#sauvegarder le modele\n",
        "def save_model(model, optimizer, train_elbos, val_elbos, epoch_time, epoch, save_dir, best_model=False):\n",
        "    if best_model:\n",
        "        path = os.path.join(save_dir, f'best_model.pth')\n",
        "    else:\n",
        "        path = os.path.join(save_dir, f'model_epoch_{epoch}.pth')\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_elbos': train_elbos,\n",
        "        'val_elbos': val_elbos,\n",
        "    }, path)\n",
        "\n",
        "    epochs = [j for j in range(epoch+1)]\n",
        "    stats = {'Epoch': epochs, 'Training Elbo': train_elbos, \"Validation Elbo\": val_elbos, \"Epoch Time\": epoch_time}\n",
        "    stats_path = os.path.join(save_dir, 'Summary_stats.csv')\n",
        "    with open(stats_path, 'w') as csvfile:\n",
        "        fieldnames = stats.keys()\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(fieldnames)\n",
        "        writer.writerows([[stats[key][j] for key in fieldnames] for j in epochs])\n",
        "\n",
        "\n",
        "\n",
        "#Entrainement du modele pour un nombre d'epoque(epochs) definie\n",
        "def train(model, optimizer, training_load, val_loader, loss_function, epochs, save_dir = os.curdir, save_interval=None, log_interval=None,\n",
        "          model_outputs_logits=True, train_samples=None, val_samples=None, random_z=None):\n",
        "    train_elbos = []\n",
        "    val_elbos = []\n",
        "    epoch_time = []\n",
        "    max_val_elbo = -1000000\n",
        "    for epoch in range(epochs):\n",
        "        stime = time.time()\n",
        "        train_elbo = epoch_training(model, optimizer, training_load, loss_function, epoch, log_interval)\n",
        "        val_elbo = epoch_validation(model, val_loader, loss_function)\n",
        "        train_elbos.append(train_elbo)\n",
        "        val_elbos.append(val_elbo)\n",
        "        epoch_time_ = time.time() - stime\n",
        "        epoch_time.append(epoch_time_)\n",
        "\n",
        "        if val_elbo > max_val_elbo:\n",
        "            max_val_elbo = val_elbo\n",
        "            if save_interval is not None:\n",
        "                save_model(model, optimizer, train_elbos, val_elbos, epoch_time, epoch, save_dir, True)\n",
        "                generer_echantillon(model, save_dir=save_dir, epoch=epoch, train_samples=train_samples,\n",
        "                                 val_samples=val_samples, random_z=random_z, model_outputs_logits=model_outputs_logits, best=True)\n",
        "\n",
        "        if save_interval is not None:\n",
        "            if epoch % save_interval == 0:\n",
        "                save_model(model, optimizer, train_elbos, val_elbos, epoch_time, epoch, save_dir, False)\n",
        "                generer_echantillon(model, save_dir=save_dir, epoch=epoch, train_samples=train_samples,\n",
        "                                 val_samples=val_samples, random_z=random_z, model_outputs_logits=model_outputs_logits)\n",
        "\n",
        "        print(f\"-> Epoch {epoch},\\t Training ELBO: {train_elbo:.2f},\\t Validation ELBO: {val_elbo:.2f},\\t \"\n",
        "              f\"Max Validation ELBO: {max_val_elbo:.2f},\\t Epoch Time: {epoch_time_:.2f} seconds\")\n",
        "    if save_interval is not None:\n",
        "        save_model(model, optimizer, train_elbos, val_elbos, epoch_time, epoch, save_dir, False)\n",
        "        generer_echantillon(model, save_dir=save_dir, epoch=epoch, train_samples=train_samples,\n",
        "                         val_samples=val_samples, random_z=random_z, model_outputs_logits=model_outputs_logits)\n",
        "    return train_elbos, val_elbos, epoch_time[-1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ezr0wr_K9LLC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Generate original samples vs reconstructed samples for the training and validation sets, plus some random samples"
      ]
    },
    {
      "metadata": {
        "id": "T8SARxM8_Ehv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def generer_echantillon(model, save_dir, epoch, train_samples, val_samples, random_z, model_outputs_logits=True, best=False):\n",
        "    with torch.no_grad():\n",
        "        train_samples = train_samples.to(device)\n",
        "        val_samples = val_samples.to(device)\n",
        "        random_z = random_z.to(device)\n",
        "\n",
        "        generated_train_samples, mean,_ = model(train_samples)\n",
        "        generated_val_samples, _, _ = model(val_samples)\n",
        "        generated_random_samples = model.decoder(random_z)\n",
        "\n",
        "        if model_outputs_logits:\n",
        "            generated_train_samples = (torch.sigmoid(generated_train_samples)).round().cpu()\n",
        "            generated_val_samples = (torch.sigmoid(generated_val_samples)).round().cpu()\n",
        "            generated_random_samples = (torch.sigmoid(generated_random_samples)).round().cpu()\n",
        "        else: \n",
        "            generated_train_samples = ((generated_train_samples + 1) / 2).cpu()\n",
        "            generated_val_samples = ((generated_val_samples + 1) / 2).cpu()\n",
        "            generated_random_samples = ((generated_random_samples + 1) / 2).cpu()\n",
        "\n",
        "    if epoch == 0:\n",
        "        if model_outputs_logits:\n",
        "            save_image(train_samples, os.path.join(save_dir, \"input_training_samples.png\"))\n",
        "            save_image(val_samples, os.path.join(save_dir, \"input_validation_samples.png\"))\n",
        "        else:  \n",
        "            save_image((train_samples + 1) / 2, os.path.join(save_dir, \"input_training_samples.png\"))\n",
        "            save_image((val_samples + 1) / 2, os.path.join(save_dir, \"input_validation_samples.png\"))\n",
        "\n",
        "    if best:\n",
        "        generated_train_path = os.path.join(save_dir, f\"generer_train_samples_best.png\")\n",
        "        generated_val_path = os.path.join(save_dir, f\"generer_val_samples_best.png\")\n",
        "        generated_random_path = os.path.join(save_dir, f\"generer_random_samples_best.png\")\n",
        "    else:\n",
        "        generated_train_path = os.path.join(save_dir, f\"generer_train_samples_{epoch}.png\")\n",
        "        generated_val_path = os.path.join(save_dir, f\"generer_val_samples_{epoch}.png\")\n",
        "        generated_random_path = os.path.join(save_dir, f\"generer_random_samples_{epoch}.png\")\n",
        "\n",
        "    save_image(generated_train_samples, generated_train_path)\n",
        "    save_image(generated_val_samples, generated_val_path)\n",
        "    save_image(generated_random_samples, generated_random_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GotJE0Tt8x14",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4219
        },
        "outputId": "aec0f181-5233-4bb7-d2cc-2630f0a15767"
      },
      "cell_type": "code",
      "source": [
        "#Chargement de la base de donnees Street View House Numbers (SVHN) venant de numero de maisons house numbers de Google Street View\n",
        "training_loader, validation_loader, test_loader = loading_dataset_SVHN('/content/data', batch_size=batch_size)\n",
        "\n",
        "save_parent_path = '/content'\n",
        "model_folder = \"Q3_VAE_MODEL_\"\n",
        "previous_folders = [f for f in os.listdir(save_parent_path) if (model_folder in f and os.path.isdir(f))]\n",
        "if not previous_folders:\n",
        "    save_dir = os.path.join(save_parent_path, model_folder + f'{1:03d}')\n",
        "else:\n",
        "    last_model = max(previous_folders)\n",
        "    i = int(last_model.replace(model_folder,\"\")) + 1\n",
        "    save_dir = os.path.join(save_parent_path, model_folder + f'{i:03d}')\n",
        "os.mkdir(save_dir)\n",
        "\n",
        "#Creons notre modele\n",
        "model = VAE()\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "loss_fn = Elbo()\n",
        "\n",
        "\n",
        "train_samples, _ = next(iter(training_loader))\n",
        "val_samples, _ = next(iter(validation_loader))\n",
        "random_z = torch.randn((batch_size, 100))\n",
        "\n",
        "\n",
        "#Entrainons notre modele\n",
        "train_elbos, val_elbos, epoch_time = train(model, optimizer, training_loader, validation_loader, loss_fn, epochs=epochs,\n",
        "                                           save_dir=save_dir, save_interval=save_interval, log_interval=log_interval, \n",
        "                                           model_outputs_logits=False, train_samples=train_samples, val_samples=val_samples, random_z=random_z)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: /content/data/train_32x32.mat\n",
            "Using downloaded and verified file: /content/data/test_32x32.mat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----> Epoch 0,\t Batch 0,\t Training ELBO: -3066.36\n",
            "----> Epoch 0,\t Batch 100,\t Training ELBO: -2995.45\n",
            "----> Epoch 0,\t Batch 200,\t Training ELBO: -2979.68\n",
            "----> Epoch 0,\t Batch 300,\t Training ELBO: -2970.53\n",
            "----> Epoch 0,\t Batch 400,\t Training ELBO: -2964.73\n",
            "----> Epoch 0,\t Batch 500,\t Training ELBO: -2959.92\n",
            "----> Epoch 0,\t Batch 600,\t Training ELBO: -2956.29\n",
            "----> Epoch 0,\t Batch 700,\t Training ELBO: -2953.55\n",
            "----> Epoch 0,\t Batch 800,\t Training ELBO: -2951.23\n",
            "----> Epoch 0,\t Batch 900,\t Training ELBO: -2949.34\n",
            "----> Epoch 0,\t Batch 1000,\t Training ELBO: -2947.75\n",
            "-> Epoch 0,\t Training ELBO: -2947.28,\t Validation ELBO: -2934.82,\t Max Validation ELBO: -2934.82,\t Epoch Time: 21.41 seconds\n",
            "----> Epoch 1,\t Batch 0,\t Training ELBO: -2943.72\n",
            "----> Epoch 1,\t Batch 100,\t Training ELBO: -2931.77\n",
            "----> Epoch 1,\t Batch 200,\t Training ELBO: -2931.94\n",
            "----> Epoch 1,\t Batch 300,\t Training ELBO: -2932.38\n",
            "----> Epoch 1,\t Batch 400,\t Training ELBO: -2932.07\n",
            "----> Epoch 1,\t Batch 500,\t Training ELBO: -2932.11\n",
            "----> Epoch 1,\t Batch 600,\t Training ELBO: -2931.92\n",
            "----> Epoch 1,\t Batch 700,\t Training ELBO: -2931.70\n",
            "----> Epoch 1,\t Batch 800,\t Training ELBO: -2931.45\n",
            "----> Epoch 1,\t Batch 900,\t Training ELBO: -2931.24\n",
            "----> Epoch 1,\t Batch 1000,\t Training ELBO: -2931.09\n",
            "-> Epoch 1,\t Training ELBO: -2931.05,\t Validation ELBO: -2931.16,\t Max Validation ELBO: -2931.16,\t Epoch Time: 18.46 seconds\n",
            "----> Epoch 2,\t Batch 0,\t Training ELBO: -2935.84\n",
            "----> Epoch 2,\t Batch 100,\t Training ELBO: -2929.43\n",
            "----> Epoch 2,\t Batch 200,\t Training ELBO: -2928.97\n",
            "----> Epoch 2,\t Batch 300,\t Training ELBO: -2929.31\n",
            "----> Epoch 2,\t Batch 400,\t Training ELBO: -2929.47\n",
            "----> Epoch 2,\t Batch 500,\t Training ELBO: -2929.43\n",
            "----> Epoch 2,\t Batch 600,\t Training ELBO: -2929.17\n",
            "----> Epoch 2,\t Batch 700,\t Training ELBO: -2929.11\n",
            "----> Epoch 2,\t Batch 800,\t Training ELBO: -2929.17\n",
            "----> Epoch 2,\t Batch 900,\t Training ELBO: -2929.12\n",
            "----> Epoch 2,\t Batch 1000,\t Training ELBO: -2929.13\n",
            "-> Epoch 2,\t Training ELBO: -2929.08,\t Validation ELBO: -2930.03,\t Max Validation ELBO: -2930.03,\t Epoch Time: 18.57 seconds\n",
            "----> Epoch 3,\t Batch 0,\t Training ELBO: -2923.40\n",
            "----> Epoch 3,\t Batch 100,\t Training ELBO: -2928.93\n",
            "----> Epoch 3,\t Batch 200,\t Training ELBO: -2928.67\n",
            "----> Epoch 3,\t Batch 300,\t Training ELBO: -2928.46\n",
            "----> Epoch 3,\t Batch 400,\t Training ELBO: -2928.23\n",
            "----> Epoch 3,\t Batch 500,\t Training ELBO: -2928.61\n",
            "----> Epoch 3,\t Batch 600,\t Training ELBO: -2928.40\n",
            "----> Epoch 3,\t Batch 700,\t Training ELBO: -2928.20\n",
            "----> Epoch 3,\t Batch 800,\t Training ELBO: -2928.33\n",
            "----> Epoch 3,\t Batch 900,\t Training ELBO: -2928.14\n",
            "----> Epoch 3,\t Batch 1000,\t Training ELBO: -2928.18\n",
            "-> Epoch 3,\t Training ELBO: -2928.24,\t Validation ELBO: -2929.41,\t Max Validation ELBO: -2929.41,\t Epoch Time: 18.57 seconds\n",
            "----> Epoch 4,\t Batch 0,\t Training ELBO: -2923.77\n",
            "----> Epoch 4,\t Batch 100,\t Training ELBO: -2926.90\n",
            "----> Epoch 4,\t Batch 200,\t Training ELBO: -2927.64\n",
            "----> Epoch 4,\t Batch 300,\t Training ELBO: -2927.54\n",
            "----> Epoch 4,\t Batch 400,\t Training ELBO: -2927.58\n",
            "----> Epoch 4,\t Batch 500,\t Training ELBO: -2927.70\n",
            "----> Epoch 4,\t Batch 600,\t Training ELBO: -2927.68\n",
            "----> Epoch 4,\t Batch 700,\t Training ELBO: -2927.66\n",
            "----> Epoch 4,\t Batch 800,\t Training ELBO: -2927.73\n",
            "----> Epoch 4,\t Batch 900,\t Training ELBO: -2927.70\n",
            "----> Epoch 4,\t Batch 1000,\t Training ELBO: -2927.73\n",
            "-> Epoch 4,\t Training ELBO: -2927.64,\t Validation ELBO: -2928.92,\t Max Validation ELBO: -2928.92,\t Epoch Time: 19.38 seconds\n",
            "----> Epoch 5,\t Batch 0,\t Training ELBO: -2909.89\n",
            "----> Epoch 5,\t Batch 100,\t Training ELBO: -2927.99\n",
            "----> Epoch 5,\t Batch 200,\t Training ELBO: -2927.94\n",
            "----> Epoch 5,\t Batch 300,\t Training ELBO: -2927.38\n",
            "----> Epoch 5,\t Batch 400,\t Training ELBO: -2927.23\n",
            "----> Epoch 5,\t Batch 500,\t Training ELBO: -2927.38\n",
            "----> Epoch 5,\t Batch 600,\t Training ELBO: -2927.13\n",
            "----> Epoch 5,\t Batch 700,\t Training ELBO: -2927.15\n",
            "----> Epoch 5,\t Batch 800,\t Training ELBO: -2927.09\n",
            "----> Epoch 5,\t Batch 900,\t Training ELBO: -2927.33\n",
            "----> Epoch 5,\t Batch 1000,\t Training ELBO: -2927.30\n",
            "-> Epoch 5,\t Training ELBO: -2927.27,\t Validation ELBO: -2928.49,\t Max Validation ELBO: -2928.49,\t Epoch Time: 18.98 seconds\n",
            "----> Epoch 6,\t Batch 0,\t Training ELBO: -2946.18\n",
            "----> Epoch 6,\t Batch 100,\t Training ELBO: -2927.99\n",
            "----> Epoch 6,\t Batch 200,\t Training ELBO: -2926.97\n",
            "----> Epoch 6,\t Batch 300,\t Training ELBO: -2927.22\n",
            "----> Epoch 6,\t Batch 400,\t Training ELBO: -2926.69\n",
            "----> Epoch 6,\t Batch 500,\t Training ELBO: -2926.57\n",
            "----> Epoch 6,\t Batch 600,\t Training ELBO: -2926.56\n",
            "----> Epoch 6,\t Batch 700,\t Training ELBO: -2926.44\n",
            "----> Epoch 6,\t Batch 800,\t Training ELBO: -2926.53\n",
            "----> Epoch 6,\t Batch 900,\t Training ELBO: -2926.82\n",
            "----> Epoch 6,\t Batch 1000,\t Training ELBO: -2926.97\n",
            "-> Epoch 6,\t Training ELBO: -2926.93,\t Validation ELBO: -2928.34,\t Max Validation ELBO: -2928.34,\t Epoch Time: 18.46 seconds\n",
            "----> Epoch 7,\t Batch 0,\t Training ELBO: -2928.59\n",
            "----> Epoch 7,\t Batch 100,\t Training ELBO: -2927.98\n",
            "----> Epoch 7,\t Batch 200,\t Training ELBO: -2927.22\n",
            "----> Epoch 7,\t Batch 300,\t Training ELBO: -2926.97\n",
            "----> Epoch 7,\t Batch 400,\t Training ELBO: -2927.16\n",
            "----> Epoch 7,\t Batch 500,\t Training ELBO: -2927.15\n",
            "----> Epoch 7,\t Batch 600,\t Training ELBO: -2927.01\n",
            "----> Epoch 7,\t Batch 700,\t Training ELBO: -2926.79\n",
            "----> Epoch 7,\t Batch 800,\t Training ELBO: -2926.60\n",
            "----> Epoch 7,\t Batch 900,\t Training ELBO: -2926.68\n",
            "----> Epoch 7,\t Batch 1000,\t Training ELBO: -2926.61\n",
            "-> Epoch 7,\t Training ELBO: -2926.66,\t Validation ELBO: -2927.99,\t Max Validation ELBO: -2927.99,\t Epoch Time: 19.30 seconds\n",
            "----> Epoch 8,\t Batch 0,\t Training ELBO: -2915.80\n",
            "----> Epoch 8,\t Batch 100,\t Training ELBO: -2925.91\n",
            "----> Epoch 8,\t Batch 200,\t Training ELBO: -2926.82\n",
            "----> Epoch 8,\t Batch 300,\t Training ELBO: -2926.86\n",
            "----> Epoch 8,\t Batch 400,\t Training ELBO: -2926.46\n",
            "----> Epoch 8,\t Batch 500,\t Training ELBO: -2926.14\n",
            "----> Epoch 8,\t Batch 600,\t Training ELBO: -2926.04\n",
            "----> Epoch 8,\t Batch 700,\t Training ELBO: -2926.26\n",
            "----> Epoch 8,\t Batch 800,\t Training ELBO: -2926.32\n",
            "----> Epoch 8,\t Batch 900,\t Training ELBO: -2926.48\n",
            "----> Epoch 8,\t Batch 1000,\t Training ELBO: -2926.41\n",
            "-> Epoch 8,\t Training ELBO: -2926.45,\t Validation ELBO: -2928.27,\t Max Validation ELBO: -2927.99,\t Epoch Time: 18.65 seconds\n",
            "----> Epoch 9,\t Batch 0,\t Training ELBO: -2930.86\n",
            "----> Epoch 9,\t Batch 100,\t Training ELBO: -2925.47\n",
            "----> Epoch 9,\t Batch 200,\t Training ELBO: -2925.35\n",
            "----> Epoch 9,\t Batch 300,\t Training ELBO: -2925.42\n",
            "----> Epoch 9,\t Batch 400,\t Training ELBO: -2925.82\n",
            "----> Epoch 9,\t Batch 500,\t Training ELBO: -2926.14\n",
            "----> Epoch 9,\t Batch 600,\t Training ELBO: -2926.21\n",
            "----> Epoch 9,\t Batch 700,\t Training ELBO: -2926.25\n",
            "----> Epoch 9,\t Batch 800,\t Training ELBO: -2926.33\n",
            "----> Epoch 9,\t Batch 900,\t Training ELBO: -2926.32\n",
            "----> Epoch 9,\t Batch 1000,\t Training ELBO: -2926.19\n",
            "-> Epoch 9,\t Training ELBO: -2926.22,\t Validation ELBO: -2927.73,\t Max Validation ELBO: -2927.73,\t Epoch Time: 19.70 seconds\n",
            "----> Epoch 10,\t Batch 0,\t Training ELBO: -2930.31\n",
            "----> Epoch 10,\t Batch 100,\t Training ELBO: -2926.32\n",
            "----> Epoch 10,\t Batch 200,\t Training ELBO: -2926.00\n",
            "----> Epoch 10,\t Batch 300,\t Training ELBO: -2925.99\n",
            "----> Epoch 10,\t Batch 400,\t Training ELBO: -2926.12\n",
            "----> Epoch 10,\t Batch 500,\t Training ELBO: -2926.21\n",
            "----> Epoch 10,\t Batch 600,\t Training ELBO: -2926.11\n",
            "----> Epoch 10,\t Batch 700,\t Training ELBO: -2926.12\n",
            "----> Epoch 10,\t Batch 800,\t Training ELBO: -2926.05\n",
            "----> Epoch 10,\t Batch 900,\t Training ELBO: -2926.14\n",
            "----> Epoch 10,\t Batch 1000,\t Training ELBO: -2926.00\n",
            "-> Epoch 10,\t Training ELBO: -2926.01,\t Validation ELBO: -2927.51,\t Max Validation ELBO: -2927.51,\t Epoch Time: 18.40 seconds\n",
            "----> Epoch 11,\t Batch 0,\t Training ELBO: -2935.49\n",
            "----> Epoch 11,\t Batch 100,\t Training ELBO: -2926.58\n",
            "----> Epoch 11,\t Batch 200,\t Training ELBO: -2925.96\n",
            "----> Epoch 11,\t Batch 300,\t Training ELBO: -2925.84\n",
            "----> Epoch 11,\t Batch 400,\t Training ELBO: -2925.59\n",
            "----> Epoch 11,\t Batch 500,\t Training ELBO: -2925.89\n",
            "----> Epoch 11,\t Batch 600,\t Training ELBO: -2925.79\n",
            "----> Epoch 11,\t Batch 700,\t Training ELBO: -2925.87\n",
            "----> Epoch 11,\t Batch 800,\t Training ELBO: -2925.97\n",
            "----> Epoch 11,\t Batch 900,\t Training ELBO: -2925.95\n",
            "----> Epoch 11,\t Batch 1000,\t Training ELBO: -2925.85\n",
            "-> Epoch 11,\t Training ELBO: -2925.91,\t Validation ELBO: -2927.30,\t Max Validation ELBO: -2927.30,\t Epoch Time: 18.33 seconds\n",
            "----> Epoch 12,\t Batch 0,\t Training ELBO: -2936.77\n",
            "----> Epoch 12,\t Batch 100,\t Training ELBO: -2925.40\n",
            "----> Epoch 12,\t Batch 200,\t Training ELBO: -2925.27\n",
            "----> Epoch 12,\t Batch 300,\t Training ELBO: -2925.04\n",
            "----> Epoch 12,\t Batch 400,\t Training ELBO: -2925.37\n",
            "----> Epoch 12,\t Batch 500,\t Training ELBO: -2925.41\n",
            "----> Epoch 12,\t Batch 600,\t Training ELBO: -2925.63\n",
            "----> Epoch 12,\t Batch 700,\t Training ELBO: -2925.69\n",
            "----> Epoch 12,\t Batch 800,\t Training ELBO: -2925.72\n",
            "----> Epoch 12,\t Batch 900,\t Training ELBO: -2925.68\n",
            "----> Epoch 12,\t Batch 1000,\t Training ELBO: -2925.65\n",
            "-> Epoch 12,\t Training ELBO: -2925.71,\t Validation ELBO: -2927.19,\t Max Validation ELBO: -2927.19,\t Epoch Time: 18.32 seconds\n",
            "----> Epoch 13,\t Batch 0,\t Training ELBO: -2918.51\n",
            "----> Epoch 13,\t Batch 100,\t Training ELBO: -2925.27\n",
            "----> Epoch 13,\t Batch 200,\t Training ELBO: -2924.99\n",
            "----> Epoch 13,\t Batch 300,\t Training ELBO: -2924.90\n",
            "----> Epoch 13,\t Batch 400,\t Training ELBO: -2925.03\n",
            "----> Epoch 13,\t Batch 500,\t Training ELBO: -2925.26\n",
            "----> Epoch 13,\t Batch 600,\t Training ELBO: -2925.34\n",
            "----> Epoch 13,\t Batch 700,\t Training ELBO: -2925.17\n",
            "----> Epoch 13,\t Batch 800,\t Training ELBO: -2925.30\n",
            "----> Epoch 13,\t Batch 900,\t Training ELBO: -2925.53\n",
            "----> Epoch 13,\t Batch 1000,\t Training ELBO: -2925.51\n",
            "-> Epoch 13,\t Training ELBO: -2925.57,\t Validation ELBO: -2927.15,\t Max Validation ELBO: -2927.15,\t Epoch Time: 19.90 seconds\n",
            "----> Epoch 14,\t Batch 0,\t Training ELBO: -2934.24\n",
            "----> Epoch 14,\t Batch 100,\t Training ELBO: -2927.21\n",
            "----> Epoch 14,\t Batch 200,\t Training ELBO: -2925.75\n",
            "----> Epoch 14,\t Batch 300,\t Training ELBO: -2926.14\n",
            "----> Epoch 14,\t Batch 400,\t Training ELBO: -2925.96\n",
            "----> Epoch 14,\t Batch 500,\t Training ELBO: -2925.65\n",
            "----> Epoch 14,\t Batch 600,\t Training ELBO: -2925.72\n",
            "----> Epoch 14,\t Batch 700,\t Training ELBO: -2925.64\n",
            "----> Epoch 14,\t Batch 800,\t Training ELBO: -2925.53\n",
            "----> Epoch 14,\t Batch 900,\t Training ELBO: -2925.50\n",
            "----> Epoch 14,\t Batch 1000,\t Training ELBO: -2925.50\n",
            "-> Epoch 14,\t Training ELBO: -2925.47,\t Validation ELBO: -2926.97,\t Max Validation ELBO: -2926.97,\t Epoch Time: 18.28 seconds\n",
            "----> Epoch 15,\t Batch 0,\t Training ELBO: -2919.93\n",
            "----> Epoch 15,\t Batch 100,\t Training ELBO: -2925.19\n",
            "----> Epoch 15,\t Batch 200,\t Training ELBO: -2925.00\n",
            "----> Epoch 15,\t Batch 300,\t Training ELBO: -2925.49\n",
            "----> Epoch 15,\t Batch 400,\t Training ELBO: -2925.12\n",
            "----> Epoch 15,\t Batch 500,\t Training ELBO: -2925.27\n",
            "----> Epoch 15,\t Batch 600,\t Training ELBO: -2925.32\n",
            "----> Epoch 15,\t Batch 700,\t Training ELBO: -2925.28\n",
            "----> Epoch 15,\t Batch 800,\t Training ELBO: -2925.26\n",
            "----> Epoch 15,\t Batch 900,\t Training ELBO: -2925.20\n",
            "----> Epoch 15,\t Batch 1000,\t Training ELBO: -2925.32\n",
            "-> Epoch 15,\t Training ELBO: -2925.31,\t Validation ELBO: -2926.89,\t Max Validation ELBO: -2926.89,\t Epoch Time: 18.62 seconds\n",
            "----> Epoch 16,\t Batch 0,\t Training ELBO: -2937.81\n",
            "----> Epoch 16,\t Batch 100,\t Training ELBO: -2926.15\n",
            "----> Epoch 16,\t Batch 200,\t Training ELBO: -2925.45\n",
            "----> Epoch 16,\t Batch 300,\t Training ELBO: -2925.81\n",
            "----> Epoch 16,\t Batch 400,\t Training ELBO: -2925.57\n",
            "----> Epoch 16,\t Batch 500,\t Training ELBO: -2925.49\n",
            "----> Epoch 16,\t Batch 600,\t Training ELBO: -2925.21\n",
            "----> Epoch 16,\t Batch 700,\t Training ELBO: -2925.02\n",
            "----> Epoch 16,\t Batch 800,\t Training ELBO: -2925.08\n",
            "----> Epoch 16,\t Batch 900,\t Training ELBO: -2925.12\n",
            "----> Epoch 16,\t Batch 1000,\t Training ELBO: -2925.22\n",
            "-> Epoch 16,\t Training ELBO: -2925.19,\t Validation ELBO: -2926.80,\t Max Validation ELBO: -2926.80,\t Epoch Time: 20.22 seconds\n",
            "----> Epoch 17,\t Batch 0,\t Training ELBO: -2933.06\n",
            "----> Epoch 17,\t Batch 100,\t Training ELBO: -2925.58\n",
            "----> Epoch 17,\t Batch 200,\t Training ELBO: -2924.77\n",
            "----> Epoch 17,\t Batch 300,\t Training ELBO: -2924.99\n",
            "----> Epoch 17,\t Batch 400,\t Training ELBO: -2925.21\n",
            "----> Epoch 17,\t Batch 500,\t Training ELBO: -2925.13\n",
            "----> Epoch 17,\t Batch 600,\t Training ELBO: -2925.04\n",
            "----> Epoch 17,\t Batch 700,\t Training ELBO: -2925.02\n",
            "----> Epoch 17,\t Batch 800,\t Training ELBO: -2925.16\n",
            "----> Epoch 17,\t Batch 900,\t Training ELBO: -2925.07\n",
            "----> Epoch 17,\t Batch 1000,\t Training ELBO: -2925.14\n",
            "-> Epoch 17,\t Training ELBO: -2925.11,\t Validation ELBO: -2926.74,\t Max Validation ELBO: -2926.74,\t Epoch Time: 20.03 seconds\n",
            "----> Epoch 18,\t Batch 0,\t Training ELBO: -2928.53\n",
            "----> Epoch 18,\t Batch 100,\t Training ELBO: -2925.42\n",
            "----> Epoch 18,\t Batch 200,\t Training ELBO: -2924.67\n",
            "----> Epoch 18,\t Batch 300,\t Training ELBO: -2924.44\n",
            "----> Epoch 18,\t Batch 400,\t Training ELBO: -2924.56\n",
            "----> Epoch 18,\t Batch 500,\t Training ELBO: -2924.84\n",
            "----> Epoch 18,\t Batch 600,\t Training ELBO: -2924.82\n",
            "----> Epoch 18,\t Batch 700,\t Training ELBO: -2924.86\n",
            "----> Epoch 18,\t Batch 800,\t Training ELBO: -2924.83\n",
            "----> Epoch 18,\t Batch 900,\t Training ELBO: -2924.99\n",
            "----> Epoch 18,\t Batch 1000,\t Training ELBO: -2925.01\n",
            "-> Epoch 18,\t Training ELBO: -2925.04,\t Validation ELBO: -2926.67,\t Max Validation ELBO: -2926.67,\t Epoch Time: 18.42 seconds\n",
            "----> Epoch 19,\t Batch 0,\t Training ELBO: -2924.92\n",
            "----> Epoch 19,\t Batch 100,\t Training ELBO: -2924.91\n",
            "----> Epoch 19,\t Batch 200,\t Training ELBO: -2925.17\n",
            "----> Epoch 19,\t Batch 300,\t Training ELBO: -2925.19\n",
            "----> Epoch 19,\t Batch 400,\t Training ELBO: -2925.21\n",
            "----> Epoch 19,\t Batch 500,\t Training ELBO: -2925.14\n",
            "----> Epoch 19,\t Batch 600,\t Training ELBO: -2925.15\n",
            "----> Epoch 19,\t Batch 700,\t Training ELBO: -2925.08\n",
            "----> Epoch 19,\t Batch 800,\t Training ELBO: -2925.03\n",
            "----> Epoch 19,\t Batch 900,\t Training ELBO: -2925.09\n",
            "----> Epoch 19,\t Batch 1000,\t Training ELBO: -2924.95\n",
            "-> Epoch 19,\t Training ELBO: -2924.92,\t Validation ELBO: -2926.84,\t Max Validation ELBO: -2926.67,\t Epoch Time: 18.53 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OY7RzTrcALuD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### !rm -rf '/content/Q3_VAE_MODEL_009'\n",
        "!rm -rf '/content/Q3_VAE_MODEL_002/representation_echantillon_pertubation'"
      ]
    },
    {
      "metadata": {
        "id": "YkwKMZwJZK5o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Question 3.2 le modele VAE"
      ]
    },
    {
      "metadata": {
        "id": "0k301aeUUEnT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#produire un nbre echantillon des echantillons aleatoire\n",
        "def generate_random_samples(model, save_dir, random_z_samples=None, epoch=-1, num_samples=200, latent_size=100,\n",
        "                            model_outputs_logits=True, samples_per_image=64, generated_random_file=\"Echant_aleatoire\"):\n",
        "    with torch.no_grad():\n",
        "        num_images = ceil(num_samples / samples_per_image)\n",
        "        for k in range(1, num_images +1):\n",
        "            if k == num_images and num_samples%samples_per_image != 0:\n",
        "                samples_per_image = num_samples%samples_per_image\n",
        "            if random_z_samples is None:\n",
        "                random_z = torch.randn((samples_per_image, latent_size))\n",
        "            else:\n",
        "                try:\n",
        "                    random_z = random_z_samples[(k-1)*samples_per_image:k*samples_per_image]\n",
        "                except:\n",
        "                    if len(random_z_samples) < k*samples_per_image:\n",
        "                        print(\"echantillon aleatoire de z est tres petit pour le nbre d'images demandes\")\n",
        "            random_z = random_z.to(device)\n",
        "            generated_random_samples = model.decoder(random_z)\n",
        "            if model_outputs_logits:\n",
        "                generated_random_samples = (torch.sigmoid(generated_random_samples)).round().cpu()\n",
        "            else: \n",
        "                generated_random_samples = ((generated_random_samples + 1) / 2).cpu()\n",
        "            generated_random_file_ = generated_random_file + f\"_epoch_{epoch}_echant_{num_samples}_{k:03d}.png\"\n",
        "            image_path = os.path.join(save_dir, generated_random_file_)\n",
        "            save_image(generated_random_samples, image_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sjOqf9PyZE1k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "2af79fd0-85ef-4cab-fe61-11d868bb7b4c"
      },
      "cell_type": "code",
      "source": [
        "random_z_samples_ = torch.randn((64, 100))\n",
        "epsilon = 1.5\n",
        "save_dir_disentangled = os.path.join(save_dir, \"echantillon_pertubation\") \n",
        "if not os.path.isdir(save_dir_disentangled):\n",
        "  os.mkdir(save_dir_disentangled)\n",
        "  generate_random_samples(model, save_dir_disentangled, random_z_samples=random_z_samples_, epoch=20, num_samples=64, latent_size=100,\n",
        "                            model_outputs_logits=False, samples_per_image=64, generated_random_file=\"sans_pertubation\")\n",
        "for i in range(50):\n",
        "  random_z_samples = random_z_samples_\n",
        "  random_z_samples[:,i] += epsilon\n",
        "  generate_random_samples(model, save_dir_disentangled, random_z_samples=random_z_samples, epoch=20, num_samples=64, latent_size=100,\n",
        "                            model_outputs_logits=False, samples_per_image=64, generated_random_file=f\"avec_pertubation_{i}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "i2kcRQ6Nhlkb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### question Q3.3 VAE"
      ]
    },
    {
      "metadata": {
        "id": "1jMJqwOyh1a-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#generons les images interpolees random_z_1 and random_z_2 avec un facteur\n",
        "def generer_echantillon_interpole(model, save_dir, alphas, interpolate_images=False, random_z_0=None, random_z_1=None, epoch=-1, num_samples=64, latent_size=100,\n",
        "                            model_outputs_logits=False, generated_random_file=\"random_interpolation\"):\n",
        "    save_dir = os.path.join(save_dir, \"Echantillon_interpolation\")\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.mkdir(save_dir)\n",
        "    with torch.no_grad():\n",
        "        if (random_z_0 is None) or (random_z_1 is None):\n",
        "            random_z_0 = torch.randn((num_samples, latent_size))\n",
        "            random_z_1 = torch.randn((num_samples, latent_size))\n",
        "        random_z_0 = random_z_0.to(device)\n",
        "        random_z_1 = random_z_1.to(device)\n",
        "        if interpolate_images:\n",
        "            generated_images_0 = model.decoder(random_z_0)\n",
        "            generated_images_1 = model.decoder(random_z_1)\n",
        "            for alpha in alphas:\n",
        "                generated_images = alpha*generated_images_0 + (1.-alpha) * generated_images_1\n",
        "                if model_outputs_logits:\n",
        "                    generated_images = (torch.sigmoid(generated_images)).round().cpu()\n",
        "                else: \n",
        "                    generated_images = ((generated_images + 1) / 2).cpu()\n",
        "                generated_random_file_ = generated_random_file + f\"_images_alpha_{alpha}.png\"\n",
        "                image_path = os.path.join(save_dir, generated_random_file_)\n",
        "                save_image(generated_images, image_path)\n",
        "        else:\n",
        "            for alpha in alphas:\n",
        "                random_z = alpha*random_z_0 + (1.-alpha)*random_z_1\n",
        "                generated_images = model.decoder(random_z)\n",
        "                if model_outputs_logits:\n",
        "                    generated_images = (torch.sigmoid(generated_images)).round().cpu()\n",
        "                else:  \n",
        "                    generated_images = ((generated_images + 1) / 2).cpu()\n",
        "                generated_random_file_ = generated_random_file + f\"_latent_alpha_{alpha}.png\"\n",
        "                image_path = os.path.join(save_dir, generated_random_file_)\n",
        "                save_image(generated_images, image_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M4SB4vn4hlHA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "dcd94d8f-05f2-4233-d882-f8cf24470c5f"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "random_z_0 = torch.randn((64, 100))\n",
        "random_z_1 = torch.randn((64, 100))\n",
        "alphas = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "generer_echantillon_interpole(model, save_dir, alphas, interpolate_images=False, random_z_0=random_z_0, random_z_1=random_z_1, \n",
        "                              epoch=20, num_samples=64, latent_size=100, model_outputs_logits=False)\n",
        "generer_echantillon_interpole(model, save_dir, alphas, interpolate_images=True, random_z_0=random_z_0, random_z_1=random_z_1, \n",
        "                              epoch=20, num_samples=64, latent_size=100, model_outputs_logits=False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "VZ9MyyYfR50Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Q3.4 valuation quantitative"
      ]
    },
    {
      "metadata": {
        "id": "x6J2ktE3WkYd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f14ae780-4ba4-4a67-8e98-5fd2422f1d57"
      },
      "cell_type": "code",
      "source": [
        "file=\"Echant_aleatoire\"\n",
        "\n",
        "for i in range(1000):\n",
        "  random_z = torch.randn((1, 100))\n",
        "  random_z = random_z.to(device)\n",
        "  random_samples = model.decoder(random_z)\n",
        "  file_ = os.path.join(save_dir, f\"eval_image_{i}.png\")\n",
        "  image_path = os.path.join(save_dir, file_)\n",
        "  save_image(random_samples, image_path)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZlA1vxrHtRGE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  !zip -r dossier1.zip  /content/Q3_VAE_MODEL_002"
      ]
    }
  ]
}