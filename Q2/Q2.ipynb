{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASS2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "avQ8jOac4jzr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import utils\n",
        "import torch.utils.data as data_utils\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.nn.modules import upsampling\n",
        "from torch.functional import F\n",
        "from torch.optim import Adam\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l1GIhT9w4pbQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data_loader(dataset_location, batch_size):\n",
        "    URL = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/\"\n",
        "    # start processing\n",
        "    def lines_to_np_array(lines):\n",
        "        return np.array([[int(i) for i in line.split()] for line in lines])\n",
        "    splitdata = []\n",
        "    for splitname in [\"train\", \"valid\", \"test\"]:\n",
        "        filename = \"binarized_mnist_%s.amat\" % splitname\n",
        "        filepath = os.path.join(dataset_location, filename)\n",
        "        utils.download_url(URL + filename, dataset_location)\n",
        "        with open(filepath) as f:\n",
        "            lines = f.readlines()\n",
        "        x = lines_to_np_array(lines).astype('float32')\n",
        "        x = x.reshape(x.shape[0], 1, 28, 28)\n",
        "        # pytorch data loader\n",
        "        dataset = data_utils.TensorDataset(torch.from_numpy(x))\n",
        "        dataset_loader = data_utils.DataLoader(x, batch_size=batch_size, shuffle=splitname == \"train\")\n",
        "        splitdata.append(dataset_loader)\n",
        "    return splitdata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VoeQQ5Iq4tMw",
        "colab_type": "code",
        "outputId": "4fcfc3b9-20f5-4717-9097-d84cbd6944c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sYQawc9Q41TI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "batch_size = 64\n",
        "n_epochs = 20\n",
        "K=200\n",
        "log_interval = 100\n",
        "lr = 3e-4\n",
        "latent_size = 100\n",
        "input_size=(1,28,28)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L9rWRmFL44sR",
        "colab_type": "code",
        "outputId": "e3ba8cbb-4a56-45f7-83ea-81a77065df57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "x_train, x_valid, x_test = get_data_loader(\"binarized_mnist\", batch_size)"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: binarized_mnist/binarized_mnist_train.amat\n",
            "Using downloaded and verified file: binarized_mnist/binarized_mnist_valid.amat\n",
            "Using downloaded and verified file: binarized_mnist/binarized_mnist_test.amat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zvoVXdwx4_qP",
        "colab_type": "code",
        "outputId": "8d44b945-5324-4abb-b6d0-73bbc95fde6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "for x in x_train:\n",
        "    plt.imshow(x[0,0])\n",
        "    break"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nivovVyS9vuGU6s8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "m97-ngj95CiA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3))\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=(2, 2), stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3))\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=(2, 2), stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=(5, 5))\n",
        "        self._mean = nn.Linear(in_features=256, out_features=100, bias=True)\n",
        "        self._logvar = nn.Linear(in_features=256, out_features=100, bias=True)\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = self.conv1(input)\n",
        "        input = self.elu(input)\n",
        "        input = self.pool1(input)\n",
        "        input = self.conv2(input)\n",
        "        input = self.elu(input)\n",
        "        input = self.pool2(input)\n",
        "        input = self.conv3(input)\n",
        "        input = self.elu(input)\n",
        "        input = input.view(input.size(0), -1)\n",
        "        mean = self._mean(input)\n",
        "        logvar = self._logvar(input)\n",
        "        return mean, logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.linear = nn.Linear(in_features=100, out_features=256, bias=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels=256, out_channels=64, kernel_size=(5, 5), padding=4)\n",
        "        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(3, 3), padding=2)\n",
        "        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(3, 3), padding=2)\n",
        "        self.conv4 = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=(3, 3), padding=2)\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = self.linear(input)\n",
        "        input = input.reshape(input.size(0), 256, 1, 1)\n",
        "        input = self.elu(input)\n",
        "        input = self.conv1(input)\n",
        "        input = self.elu(input)\n",
        "        input = self.upsample1(input)\n",
        "        input = self.conv2(input)\n",
        "        input = self.elu(input)\n",
        "        input = self.upsample2(input)\n",
        "        input = self.conv3(input)\n",
        "        input = self.elu(input)\n",
        "        return torch.sigmoid(self.conv4(input))\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, input):\n",
        "        mean, logvar = self.encoder(input)\n",
        "        epsilon = torch.randn_like(logvar)\n",
        "        input = mean + torch.exp(logvar / 2) * epsilon \n",
        "        return self.decoder(input), mean, logvar\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-syepxrA5HUg",
        "colab_type": "code",
        "outputId": "74c40cc9-3036-41bd-aa6e-2b026901a279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "cell_type": "code",
      "source": [
        "model = VAE()\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "summary(model, input_size=input_size)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 26, 26]             320\n",
            "               ELU-2           [-1, 32, 26, 26]               0\n",
            "         AvgPool2d-3           [-1, 32, 13, 13]               0\n",
            "            Conv2d-4           [-1, 64, 11, 11]          18,496\n",
            "               ELU-5           [-1, 64, 11, 11]               0\n",
            "         AvgPool2d-6             [-1, 64, 5, 5]               0\n",
            "            Conv2d-7            [-1, 256, 1, 1]         409,856\n",
            "               ELU-8            [-1, 256, 1, 1]               0\n",
            "            Linear-9                  [-1, 100]          25,700\n",
            "           Linear-10                  [-1, 100]          25,700\n",
            "          Encoder-11     [[-1, 100], [-1, 100]]               0\n",
            "           Linear-12                  [-1, 256]          25,856\n",
            "              ELU-13            [-1, 256, 1, 1]               0\n",
            "           Conv2d-14             [-1, 64, 5, 5]         409,664\n",
            "              ELU-15             [-1, 64, 5, 5]               0\n",
            "         Upsample-16           [-1, 64, 10, 10]               0\n",
            "           Conv2d-17           [-1, 32, 12, 12]          18,464\n",
            "              ELU-18           [-1, 32, 12, 12]               0\n",
            "         Upsample-19           [-1, 32, 24, 24]               0\n",
            "           Conv2d-20           [-1, 16, 26, 26]           4,624\n",
            "              ELU-21           [-1, 16, 26, 26]               0\n",
            "           Conv2d-22            [-1, 1, 28, 28]             145\n",
            "          Decoder-23            [-1, 1, 28, 28]               0\n",
            "================================================================\n",
            "Total params: 938,825\n",
            "Trainable params: 938,825\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.90\n",
            "Params size (MB): 3.58\n",
            "Estimated Total Size (MB): 4.48\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "lMBt6MmU5W-I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#src=https://github.com/pytorch/examples/blob/master/vae/main.py\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
        "    # see Appendix B from VAE paper:\n",
        "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "    # https://arxiv.org/abs/1312.6114\n",
        "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp())\n",
        "\n",
        "    return BCE + KLD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aV2fFGFe5Zto",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#src=https://github.com/pytorch/examples/blob/master/vae/main.py\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, data in enumerate(x_train):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(x_train.dataset),\n",
        "                100. * batch_idx / len(x_train),\n",
        "                -loss.item() / len(data)))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, -train_loss / len(x_train.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uz4EzziV5cfA",
        "colab_type": "code",
        "outputId": "b8b31749-276f-4a85-8cb3-d13fc842fa14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3308
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "        train(epoch)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/50000 (0%)]\tLoss: -557.901978\n",
            "Train Epoch: 0 [6400/50000 (13%)]\tLoss: -218.308334\n",
            "Train Epoch: 0 [12800/50000 (26%)]\tLoss: -189.404953\n",
            "Train Epoch: 0 [19200/50000 (38%)]\tLoss: -175.412689\n",
            "Train Epoch: 0 [25600/50000 (51%)]\tLoss: -164.643707\n",
            "Train Epoch: 0 [32000/50000 (64%)]\tLoss: -160.002869\n",
            "Train Epoch: 0 [38400/50000 (77%)]\tLoss: -152.281952\n",
            "Train Epoch: 0 [44800/50000 (90%)]\tLoss: -143.354248\n",
            "====> Epoch: 0 Average loss: -186.7176\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: -141.747772\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: -134.328323\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: -134.085159\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: -129.190674\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: -118.624695\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: -124.162689\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: -115.339905\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: -111.903313\n",
            "====> Epoch: 1 Average loss: -127.8042\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: -111.254578\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: -124.442612\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: -109.389755\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: -112.618134\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: -114.558052\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: -111.772903\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: -103.529480\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: -114.792099\n",
            "====> Epoch: 2 Average loss: -112.1602\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: -108.281319\n",
            "Train Epoch: 3 [6400/50000 (13%)]\tLoss: -105.659775\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: -105.379669\n",
            "Train Epoch: 3 [19200/50000 (38%)]\tLoss: -109.445389\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: -105.061867\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: -107.064636\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: -106.742508\n",
            "Train Epoch: 3 [44800/50000 (90%)]\tLoss: -107.594070\n",
            "====> Epoch: 3 Average loss: -106.5729\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: -110.596375\n",
            "Train Epoch: 4 [6400/50000 (13%)]\tLoss: -96.423439\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: -95.589859\n",
            "Train Epoch: 4 [19200/50000 (38%)]\tLoss: -106.430595\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: -97.194626\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: -101.617043\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: -107.495682\n",
            "Train Epoch: 4 [44800/50000 (90%)]\tLoss: -99.885483\n",
            "====> Epoch: 4 Average loss: -103.6035\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: -106.383209\n",
            "Train Epoch: 5 [6400/50000 (13%)]\tLoss: -101.556007\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: -104.783310\n",
            "Train Epoch: 5 [19200/50000 (38%)]\tLoss: -104.120743\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: -103.541328\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: -96.010056\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: -93.854012\n",
            "Train Epoch: 5 [44800/50000 (90%)]\tLoss: -99.349716\n",
            "====> Epoch: 5 Average loss: -101.5869\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLoss: -100.913292\n",
            "Train Epoch: 6 [6400/50000 (13%)]\tLoss: -92.588440\n",
            "Train Epoch: 6 [12800/50000 (26%)]\tLoss: -95.924080\n",
            "Train Epoch: 6 [19200/50000 (38%)]\tLoss: -106.398758\n",
            "Train Epoch: 6 [25600/50000 (51%)]\tLoss: -99.141251\n",
            "Train Epoch: 6 [32000/50000 (64%)]\tLoss: -98.917404\n",
            "Train Epoch: 6 [38400/50000 (77%)]\tLoss: -101.195099\n",
            "Train Epoch: 6 [44800/50000 (90%)]\tLoss: -105.767143\n",
            "====> Epoch: 6 Average loss: -100.2004\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLoss: -107.902649\n",
            "Train Epoch: 7 [6400/50000 (13%)]\tLoss: -101.414200\n",
            "Train Epoch: 7 [12800/50000 (26%)]\tLoss: -102.620323\n",
            "Train Epoch: 7 [19200/50000 (38%)]\tLoss: -101.465103\n",
            "Train Epoch: 7 [25600/50000 (51%)]\tLoss: -96.810738\n",
            "Train Epoch: 7 [32000/50000 (64%)]\tLoss: -94.977432\n",
            "Train Epoch: 7 [38400/50000 (77%)]\tLoss: -104.231682\n",
            "Train Epoch: 7 [44800/50000 (90%)]\tLoss: -98.197388\n",
            "====> Epoch: 7 Average loss: -99.1789\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLoss: -93.469521\n",
            "Train Epoch: 8 [6400/50000 (13%)]\tLoss: -100.039047\n",
            "Train Epoch: 8 [12800/50000 (26%)]\tLoss: -104.588730\n",
            "Train Epoch: 8 [19200/50000 (38%)]\tLoss: -97.541451\n",
            "Train Epoch: 8 [25600/50000 (51%)]\tLoss: -96.039505\n",
            "Train Epoch: 8 [32000/50000 (64%)]\tLoss: -100.790337\n",
            "Train Epoch: 8 [38400/50000 (77%)]\tLoss: -100.643517\n",
            "Train Epoch: 8 [44800/50000 (90%)]\tLoss: -97.494827\n",
            "====> Epoch: 8 Average loss: -98.2668\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLoss: -93.794609\n",
            "Train Epoch: 9 [6400/50000 (13%)]\tLoss: -101.637901\n",
            "Train Epoch: 9 [12800/50000 (26%)]\tLoss: -98.789841\n",
            "Train Epoch: 9 [19200/50000 (38%)]\tLoss: -94.685005\n",
            "Train Epoch: 9 [25600/50000 (51%)]\tLoss: -98.989365\n",
            "Train Epoch: 9 [32000/50000 (64%)]\tLoss: -97.469437\n",
            "Train Epoch: 9 [38400/50000 (77%)]\tLoss: -100.654343\n",
            "Train Epoch: 9 [44800/50000 (90%)]\tLoss: -93.059570\n",
            "====> Epoch: 9 Average loss: -97.6210\n",
            "Train Epoch: 10 [0/50000 (0%)]\tLoss: -95.225075\n",
            "Train Epoch: 10 [6400/50000 (13%)]\tLoss: -93.066772\n",
            "Train Epoch: 10 [12800/50000 (26%)]\tLoss: -92.087082\n",
            "Train Epoch: 10 [19200/50000 (38%)]\tLoss: -96.537369\n",
            "Train Epoch: 10 [25600/50000 (51%)]\tLoss: -98.680717\n",
            "Train Epoch: 10 [32000/50000 (64%)]\tLoss: -95.982918\n",
            "Train Epoch: 10 [38400/50000 (77%)]\tLoss: -99.966858\n",
            "Train Epoch: 10 [44800/50000 (90%)]\tLoss: -96.745041\n",
            "====> Epoch: 10 Average loss: -96.9868\n",
            "Train Epoch: 11 [0/50000 (0%)]\tLoss: -96.777069\n",
            "Train Epoch: 11 [6400/50000 (13%)]\tLoss: -95.387390\n",
            "Train Epoch: 11 [12800/50000 (26%)]\tLoss: -96.727402\n",
            "Train Epoch: 11 [19200/50000 (38%)]\tLoss: -93.415344\n",
            "Train Epoch: 11 [25600/50000 (51%)]\tLoss: -96.437851\n",
            "Train Epoch: 11 [32000/50000 (64%)]\tLoss: -97.969902\n",
            "Train Epoch: 11 [38400/50000 (77%)]\tLoss: -95.927071\n",
            "Train Epoch: 11 [44800/50000 (90%)]\tLoss: -102.468117\n",
            "====> Epoch: 11 Average loss: -96.4716\n",
            "Train Epoch: 12 [0/50000 (0%)]\tLoss: -92.663330\n",
            "Train Epoch: 12 [6400/50000 (13%)]\tLoss: -96.986160\n",
            "Train Epoch: 12 [12800/50000 (26%)]\tLoss: -98.254791\n",
            "Train Epoch: 12 [19200/50000 (38%)]\tLoss: -96.178162\n",
            "Train Epoch: 12 [25600/50000 (51%)]\tLoss: -92.377548\n",
            "Train Epoch: 12 [32000/50000 (64%)]\tLoss: -97.201530\n",
            "Train Epoch: 12 [38400/50000 (77%)]\tLoss: -97.187233\n",
            "Train Epoch: 12 [44800/50000 (90%)]\tLoss: -96.962776\n",
            "====> Epoch: 12 Average loss: -96.0602\n",
            "Train Epoch: 13 [0/50000 (0%)]\tLoss: -94.709084\n",
            "Train Epoch: 13 [6400/50000 (13%)]\tLoss: -95.299118\n",
            "Train Epoch: 13 [12800/50000 (26%)]\tLoss: -94.940102\n",
            "Train Epoch: 13 [19200/50000 (38%)]\tLoss: -95.817116\n",
            "Train Epoch: 13 [25600/50000 (51%)]\tLoss: -91.403343\n",
            "Train Epoch: 13 [32000/50000 (64%)]\tLoss: -94.472862\n",
            "Train Epoch: 13 [38400/50000 (77%)]\tLoss: -96.330124\n",
            "Train Epoch: 13 [44800/50000 (90%)]\tLoss: -101.386421\n",
            "====> Epoch: 13 Average loss: -95.6546\n",
            "Train Epoch: 14 [0/50000 (0%)]\tLoss: -91.414528\n",
            "Train Epoch: 14 [6400/50000 (13%)]\tLoss: -97.071259\n",
            "Train Epoch: 14 [12800/50000 (26%)]\tLoss: -97.863258\n",
            "Train Epoch: 14 [19200/50000 (38%)]\tLoss: -95.834892\n",
            "Train Epoch: 14 [25600/50000 (51%)]\tLoss: -96.547211\n",
            "Train Epoch: 14 [32000/50000 (64%)]\tLoss: -97.601059\n",
            "Train Epoch: 14 [38400/50000 (77%)]\tLoss: -94.777573\n",
            "Train Epoch: 14 [44800/50000 (90%)]\tLoss: -92.407303\n",
            "====> Epoch: 14 Average loss: -95.2533\n",
            "Train Epoch: 15 [0/50000 (0%)]\tLoss: -94.142220\n",
            "Train Epoch: 15 [6400/50000 (13%)]\tLoss: -94.101166\n",
            "Train Epoch: 15 [12800/50000 (26%)]\tLoss: -90.459282\n",
            "Train Epoch: 15 [19200/50000 (38%)]\tLoss: -94.298309\n",
            "Train Epoch: 15 [25600/50000 (51%)]\tLoss: -95.458267\n",
            "Train Epoch: 15 [32000/50000 (64%)]\tLoss: -95.647926\n",
            "Train Epoch: 15 [38400/50000 (77%)]\tLoss: -99.928116\n",
            "Train Epoch: 15 [44800/50000 (90%)]\tLoss: -102.407005\n",
            "====> Epoch: 15 Average loss: -94.9101\n",
            "Train Epoch: 16 [0/50000 (0%)]\tLoss: -95.803223\n",
            "Train Epoch: 16 [6400/50000 (13%)]\tLoss: -96.825912\n",
            "Train Epoch: 16 [12800/50000 (26%)]\tLoss: -96.748726\n",
            "Train Epoch: 16 [19200/50000 (38%)]\tLoss: -96.264610\n",
            "Train Epoch: 16 [25600/50000 (51%)]\tLoss: -99.182625\n",
            "Train Epoch: 16 [32000/50000 (64%)]\tLoss: -93.357330\n",
            "Train Epoch: 16 [38400/50000 (77%)]\tLoss: -91.596039\n",
            "Train Epoch: 16 [44800/50000 (90%)]\tLoss: -90.477272\n",
            "====> Epoch: 16 Average loss: -94.5836\n",
            "Train Epoch: 17 [0/50000 (0%)]\tLoss: -98.147797\n",
            "Train Epoch: 17 [6400/50000 (13%)]\tLoss: -91.110474\n",
            "Train Epoch: 17 [12800/50000 (26%)]\tLoss: -94.246841\n",
            "Train Epoch: 17 [19200/50000 (38%)]\tLoss: -87.553596\n",
            "Train Epoch: 17 [25600/50000 (51%)]\tLoss: -94.967735\n",
            "Train Epoch: 17 [32000/50000 (64%)]\tLoss: -94.095032\n",
            "Train Epoch: 17 [38400/50000 (77%)]\tLoss: -98.134186\n",
            "Train Epoch: 17 [44800/50000 (90%)]\tLoss: -89.948730\n",
            "====> Epoch: 17 Average loss: -94.3533\n",
            "Train Epoch: 18 [0/50000 (0%)]\tLoss: -98.717651\n",
            "Train Epoch: 18 [6400/50000 (13%)]\tLoss: -96.029846\n",
            "Train Epoch: 18 [12800/50000 (26%)]\tLoss: -93.565559\n",
            "Train Epoch: 18 [19200/50000 (38%)]\tLoss: -97.103828\n",
            "Train Epoch: 18 [25600/50000 (51%)]\tLoss: -97.912712\n",
            "Train Epoch: 18 [32000/50000 (64%)]\tLoss: -97.665268\n",
            "Train Epoch: 18 [38400/50000 (77%)]\tLoss: -98.135239\n",
            "Train Epoch: 18 [44800/50000 (90%)]\tLoss: -92.947426\n",
            "====> Epoch: 18 Average loss: -94.0888\n",
            "Train Epoch: 19 [0/50000 (0%)]\tLoss: -87.770332\n",
            "Train Epoch: 19 [6400/50000 (13%)]\tLoss: -97.596298\n",
            "Train Epoch: 19 [12800/50000 (26%)]\tLoss: -90.741844\n",
            "Train Epoch: 19 [19200/50000 (38%)]\tLoss: -95.700775\n",
            "Train Epoch: 19 [25600/50000 (51%)]\tLoss: -89.762100\n",
            "Train Epoch: 19 [32000/50000 (64%)]\tLoss: -92.152512\n",
            "Train Epoch: 19 [38400/50000 (77%)]\tLoss: -93.595917\n",
            "Train Epoch: 19 [44800/50000 (90%)]\tLoss: -90.668404\n",
            "====> Epoch: 19 Average loss: -93.8645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "deH4EAyMK5B0",
        "colab_type": "code",
        "outputId": "c877572e-464f-4fd8-8152-849b99155dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "cell_type": "code",
      "source": [
        "torch.save(model, '/content/model.pt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type VAE. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-BrdRFvhK7ms",
        "colab_type": "code",
        "outputId": "7b6d2745-38fc-4f62-abe6-dc6a987fc835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model=torch.load('/content/model.pt')\n",
        "model.eval()\n",
        "loss = 0\n",
        "for batch_idx, data in enumerate(x_valid):\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "loss = loss/len(x_valid.dataset)\n",
        "print('Valid - average per-instance ELBO: {} '.format(-loss))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Valid - average per-instance ELBO: -94.52338081054687 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qVoGJed3Dtly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "d6a7f115-5488-4b30-ebc8-84e33a51b27f"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model=torch.load('/content/model.pt')\n",
        "model.eval()\n",
        "loss = 0\n",
        "losses=[]\n",
        "for batch_idx, data in enumerate(x_test):\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "            losses.append(loss)\n",
        "loss = loss/len(x_valid.dataset)\n",
        "print('Test - average per-instance ELBO: {} '.format(-loss))"
      ],
      "execution_count": 463,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test - average per-instance ELBO: -93.92628044433594 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d35nLp-GZlyw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#param1: K  the number of importance samples\n",
        "#param2: mean\n",
        "#param3: std\n",
        "#return importance samples of size (M,K,L)\n",
        "def get_samples(K, mu, std):\n",
        "    mu = mu.unsqueeze(1).expand(-1, K, -1) \n",
        "    std = std.unsqueeze(1).expand(-1, K, -1)    \n",
        "    return torch.normal(mu, std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eTTOKwvyZpfw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Evaluating log-likelihood with Variational Autoencoders \n",
        "#param1: trained  model\n",
        "#param2: tensor x of data (M,D)\n",
        "#param3: tensor z_samples ( M, K, L)\n",
        "#return (log p(x_), ..., log p(x_M))\n",
        "def batch_log_px(model, x, z): \n",
        "        with torch.no_grad():\n",
        "              M = x.shape[0]\n",
        "              D = x.shape[1]\n",
        "              K = z.shape[1]\n",
        "              L = z.shape[2]\n",
        "              mu, logvar = model.encoder(x.reshape((M, 1,28,28)))\n",
        "              std = torch.exp(logvar / 2)\n",
        "              mu = mu.unsqueeze(1).expand(-1, K, -1) \n",
        "              std = std.unsqueeze(1).expand(-1, K, -1)\n",
        "              #q(z|x)\n",
        "              qz_x=(1/torch.sqrt(2*math.pi*std**2))*torch.exp(((z - mu)/std)**2)\n",
        "              log_qz_x=torch.sum(torch.log(qz_x), -1)\n",
        "              #p(z)\n",
        "              pz=(1/math.sqrt(2*math.pi))*torch.exp(z**2)\n",
        "              log_pz=torch.sum(torch.log(pz), -1)\n",
        "              #p(x|z)\n",
        "              recon_x = model.decoder(z.reshape((M*K, L)))\n",
        "              recon_x= recon_x.reshape((M, K, D))\n",
        "              log_px_z = torch.sum((x.unsqueeze(1).expand(-1, K, -1) * (recon_x).log() + (1 - x.unsqueeze(1).expand(-1, K, -1)) * (1 - recon_x).log()),-1)\n",
        "              #log p(x) = log(1/K) + log(sum(exp(log p(x|z)+log p(z)-log q(z|x))))\n",
        "              log_terms = log_px_z + log_pz - log_qz_x\n",
        "              max_log,_= log_terms.max(dim=-1, keepdim=True)\n",
        "              log_px=-np.log(K)+max_log+torch.log(torch.sum(torch.exp(log_terms-max_log)))\n",
        "        return log_px.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xKRSlx0zzIeF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#return (log p(x_1), ..., log p(x_M))\n",
        "def get_log_px_dataset(data_loader, K):\n",
        "    log_px_all = []\n",
        "    for i, x in enumerate(data_loader):\n",
        "        x = x.to(device)\n",
        "        mu, logvar = model.encoder(x)\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        z = get_samples(K,mu,std)\n",
        "        log_px_all.append(batch_log_px(model,x.reshape((x.shape[0], -1)),z).cpu().numpy())        \n",
        "    return log_px_all"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RQ3cNQUtgdVf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "29a2b9bc-daf8-4ca4-a264-00b782175298"
      },
      "cell_type": "code",
      "source": [
        "log_px_valid = get_log_px_dataset(x_valid, 200)"
      ],
      "execution_count": 474,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "oDfLVXgsJLnF",
        "colab_type": "code",
        "outputId": "a247ef4b-58b3-46f9-f29e-4a7cc0cc3c63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print('Validation -  log-likelihood: {} '.format(np.mean(log_px_valid)))\n"
      ],
      "execution_count": 476,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation -  log-likelihood: -72.93242645263672 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6am7xpyER7Lr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "UvAhj7sLMhfc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "49aca905-f1e4-4eed-e17d-8296a059069b"
      },
      "cell_type": "code",
      "source": [
        "log_px_test = get_log_px_dataset(x_test, 200)"
      ],
      "execution_count": 477,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "qwG_8Sy-O8HD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d3acc038-9646-4876-ae46-286697ed9640"
      },
      "cell_type": "code",
      "source": [
        "print('Test -  log-likelihood: {} '.format(np.mean(log_px_test)))\n"
      ],
      "execution_count": 478,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test -  log-likelihood: -72.61022186279297 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}